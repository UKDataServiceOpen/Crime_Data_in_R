---
title: "Crime mapping in R (Topics 1-4)"
author: "Nadia Kennar"
date: "25/10/2021"
output: html_notebook
toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# for data reading/manipulation 
library(dplyr)
library(tidyr)
library(readr)
library(tibble)
library(janitor)
# for spatial data and gis
library(sf)
library(ggmap)
library(ggplot2)
library(ggspatial)
library(ggspatial)
library(spdep)
library(leaflet) 
library(RColorBrewer)
library(tmap)

```



# Prerequisite 

This workshop is suitable for those beginner to intermediate in R. It requires you know how to set your working directory, how to read data into R.

Using open source police recorded crime statistics this workshop will demonstrate how to map crime data in R using sf and ggplot. More specifically looking at the area of Surrey we will 

  1) briefly explore the crime data and introduce key topics in spatial data
  2) demonstrate how to join crime data to shapefiles and how to map data 
  3) identify how to map and calculate crime rate 


The datasets needed in this workshop include crime data, population statistics and shapefiles. Information on how to download these will be available in R file names 'downloading the data' but feel free to obtain these via git *add link*. Ensure these are loaded into your environment before starting the workshop




# Topic 1: - Intro to spatial data 

## Downloading the crime data 

 *https://data.police.uk/*. 

Select January 2019 to December 2019, from the Surrey and click 'Include Crime Data'. Download and unzip the data into your working directory

Read in just the January 2019 month of data


```{r crime data}
#unzip(file.choose())
crime01_19 <- read_csv("Data/2020-08/2020-08-surrey-street.csv") %>% 
  janitor::clean_names() 

#explore variables
glimpse(crime01_19)

```

Points, lines and polygon 

- Our coordinate variables (the latitude and longitude) are known as point data 
- The 'location' variable represents the line. This is normally define by a street or junction 
- The 'lsoa name' represent our polygon (borough, wards, districts etc). LSOA refers to the Lower Layer Super Output Areas which are a unit measure in census geography 




## Briefly explore using ggmap and ggplot (areal plot)

The basic idea driving ggmap is to take a downloaded map image, plot it as a context layer using
ggplot2, and then plot additional content layers of data, statistics, or models on top of the map.

In ggmap, downloading a map as an image and formatting the image for plotting is done with the
get_map function. More specifically, get_map is a wrapper function


```{r using ggmap}

## Ariel Map of Surrey
qmplot(longitude, latitude, data = crime01_19, colour = crime_type, size = I(3), darken = .3)



## Lets just say you were interested in a specific area (in this example we will use Crawley 002B)

## Ariel Map of Crawley 002B
geocode("Crawley 002B")
Crawley <- c(long = -0.152210, lat = 51.15813)
map <- get_map(Crawley, zoom = 13, scale = 1)
ggmap(map)

ggmap(map) +
  geom_point(aes(longitude, latitude), data = crime01_19) 


## Colour the Crime Type

ggmap(map) +
  geom_point(aes(longitude, latitude, colour = crime_type), data = crime01_19) 

ggmap(map) +
  geom_point(aes(longitude, latitude, size = crime_type, colour = crime_type), data = crime01_19) 



```





## Simple Features and Projection methods

Simple Features is a common R language, also known as sf, that allow you to handle and manipulate the UoA (points, lines and polyons). Simple Features allows you store spatial objects

Features refers to the property that linestring and polygons are built from points by straight line segments. 

One of the fastest growing packages in this area is [sf](https://github.com/r-spatial/sf), which gives you access to a whole host of features and functions for use with spatial data, including visualisation. html) to spatial data out there. For this exercise, we'll keep things simple, and focus on how to use sf to make spatial data visualisations in combination with ggplot. Should you want to know more, or would like additional resources on using spatial data in R, please do not hesitate to ask!


CRS and Projection: 

CRS are  is a coordinate-based local, regional or global system used to locate geographical entities. A spatial reference system defines a specific map projection, as well as transformations between different spatial reference systems. […] spatial reference systems can be referred to using a SRID integer, including EPSG codes

In short "Projection methods allow us to move move from the 3D to the 2D, CRS allow us to identify specific locations within these 

There are thousands of CRS, the most common being BNG and the WGS 84 

Each crs has an ESPG identifier
i.e. the BNG = 27700 (British National Grid)
i.e. the WGS 84 is 4326 (World Geodetic System)
i.e. the ETRS 1980 = 3035 (European Terrestial Reference System)


First step is to transform you ordinary data into an sf object using 'st_as_sf' - which consumes are latitude and longitutde in a geometry attribute


```{r simple features }
st_crs(crime01_19)   # to check the crs

sf <- st_as_sf(crime01_19,                                
                      coords = c("longitude", "latitude"),
                      crs = 4326, 
                      na.fail = FALSE)
st_crs(sf)

```


Other functions 

- agr (atribute-geometry-relationship) = character vector. 
- Specifies for each non-geometry attribute column how it relates to the geometry, and can have one of following values: "constant", "aggregate", "identity". "constant" is used for attributes that are constant throughout the geometry (e.g. land use), "aggregate" where the attribute is an aggregate value over the geometry (e.g. population density or population count), "identity" when the attributes uniquely identifies the geometry of particular "thing", such as a building ID or a city name. The default value, NA_agr_, implies we don't know.






## Mapping point data 


Now we have an sf object which contains point-level, spatially sensitive data about Crime in Surrey 2019, We can now create a basic point map of these


```{r}

#### Plot the point data
ggplot() + 
  geom_sf(data = sf)


#### Colour the different crime type
ggplot() + 
  geom_sf(data = sf, aes(fill = crime_type, col = crime_type)) + 
  labs(title = "Crime Count in Surrey", 
       subtitle = "January 2019", 
       caption = "Police Recorded Crime Statistics")


#### Reference map 
ggplot() + 
  annotation_map_tile() +
  geom_sf(data = sf)


#### Sub-setting for just ASB 
asb <- subset(sf, crime_type == "Anti-social behaviour") %>% 
  select(-c(1, 9, 10))

ggplot() +
  annotation_map_tile() +
  geom_sf(data = asb)



```






### Activity 1

```{r}
#### Activity 1 - How does this compare to 'drugs'?
#Subset the data for the those crime types recorded as 'drugs', create this into a new object like we did for ASB and name it 'drugs' 
#Using ggplot plot the point data over a base map (reference map)


## Solution 

drugs <- subset(sf, crime_type == "Drugs") %>%  
  select(-c(1, 9, 10))

ggplot() +
  annotation_map_tile() +
  geom_sf(data = drugs) 

```












\newpage 


# Topic 2: - Shapefiles 

What are Shapefiles? 

They represent a geospatial vector that is used for GIS software. Shapefiles store both geogrpahic location and its associated attribute information 

The Shapefile format stores the data as primitive geometric shapes like points, lines, and polygons. These shapes, together with data attributes that are linked to each shape, create the representation of the geographic data.

They contain four mandatory file extensions (.shx, .shp, .dbf and the .prj). 
- The .shp contains the geometry data (a 2D axis ordering of coordinate data)
- The .shx contains the positional index of the feature geometry 
- The .dbf contins the attributes for each shape
- The .prj contains the cs and projection information


- Mention crime data and how research in criminology tend to use the LSOA as the main census geography 




Read in the Shapefile for 'Surrey Heath' 

```{r read and plot the boundary for surrey heath}
 
shp_file <- st_read("Data/Shapefile/england_lsoa_2011.shp")


## Plot the Shapefile 
ggplot() + 
  geom_sf(data = shp_file)

```



The original crime01_19 data set contains the individual count of reported crime types across LSOAS, therefore the LSOAs are repeated multiple times. This is because you would expect to see multiple crime counts in one LSOA

In order to highlight how many crimes have occurred in each LSOA, we can count the crimes per LSOA and obtained grouped statistics


```{r crimes per lsoa}
crimes_grouped_by_lsoa <- crime01_19 %>%
  group_by(lsoa_code) %>%
  summarise(count=n())
```


In our new object you will see two variables, the LSOA and the count of crime in each one.  

We can now join the Shapefile (the geospatial vector) and the crimes_grouped_by_losa (the aggregated data)

To join the crimes per lsoa to the shapefile we can use the left_join function that returns all the rows of the table on the left side of the join and matching rows for the table on the right side of join.



```{r merge the data}

surrey_lsoa <- left_join(shp_file, crimes_grouped_by_lsoa, by = c("code" = "lsoa_code"))

st_geometry_type(surrey_lsoa)    #view the geometery type 
st_bbox(surrey_lsoa)             #obtains the objects value as specific units 

#The spatial extent of a shapefile or R spatial object represents the geographic “edge” or location that is the #furthest north, south east and west. Thus is represents the overall geographic coverage of the spatial object. #Image Source: National Ecological Observatory Network (NEON).


#map the data
ggplot() + 
  annotation_map_tile() + 
  geom_sf(data = surrey_lsoa, aes(fill = count), alpha = 0.5) + 
  scale_fill_gradient2(name ="Number of crimes")

```



## Plotting via the 'tmap' package


```{r}
library(tmap)
tm_shape(surrey_lsoa) + 
  tm_fill("count") + 
  tm_borders(alpha = 0.5)
```



How can we better visualise counts? Count data does not equally represent the population distribution at hand, tmaps allows you to add different 'styles' 

The different styles result in different clustering mechanism, tmaps have available on 'jenks'or 'Standard Deviation' 


In this example I've used 'kmeans'. k-means clustering is a method of vector quantisation, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.

///

```{r}
tm_shape(surrey_lsoa) + 
  tm_fill("count", style = "kmeans") + 
  tm_borders(alpha = 0.3)

tm_shape(surrey_lsoa) + 
  tm_fill("count", style = "jenks") + 
  tm_borders(alpha = 0.3)

tm_shape(surrey_lsoa) + 
  tm_fill("count", style = "sd") + 
  tm_borders(alpha = 0.3)



```




## Map Layouts - additional features of tmap


```{r}

## map style 

tm_shape(surrey_lsoa) + 
  tm_fill("count", style = "sd") + 
  tm_borders(alpha = 0.3) + 
  tmap_style("col_blind")


## map legends

tm_shape(surrey_lsoa)+
  tm_fill("count", 
          style = "quantile", 
          palette = "Blues", 
          legend.hist = TRUE, 
          legend.is.portrait = TRUE,
          legend.hist.z = 0.1) +
  tm_layout(legend.height = 0.45, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
  tm_borders(alpha = 0.5)


## compass, scale bar and grid

tm_shape(surrey_lsoa)+
  tm_fill("count", 
          style = "quantile", 
          palette = "Blues", 
          legend.hist = TRUE, 
          legend.is.portrait = TRUE,
          legend.hist.z = 0.1) +
  tm_layout(legend.height = 0.45, 
            legend.width = 0.35,
            legend.outside = FALSE,
            legend.position = c("right", "bottom"),
            frame = FALSE) +
  tm_borders(alpha = 0.5) +
  tm_compass(type="8star", size = 2) +   #compass 
  tm_scale_bar(width = 0.15) +           #scale bar 
  tm_grid()                              #grid

```



## using categorical variables (tm_facets)


```{r}
tm_shape(surrey_lsoa) +
  tm_fill("count",
          style = "quantile",
          palette = "Blues",
          thres.poly = 0) + 
  tm_facets(by="name", 
            free.coords=TRUE, 
            drop.shapes=TRUE) +
  tm_layout(legend.show = FALSE,
            title.position = c("center", "center"), 
            title.size = 20) +
  tm_borders(alpha = 0.5)
```



## Activity 2 and break 


```{r Activity 2}

#1) Explore some of the different classification methods such as "bclust"
#   To get help on the different methods available use *??tmap-package* or search in the help tab 

#2) Head to mentimeter meter to answer some recap questions!


```






# Topic 3: - Crime Rate vs Crime Count 

Count data is not entirely accurate of population density. Whilst the above might help us identify interesting patterns, point-level open crime data is rarely used in isolation for detailed analysis. 

For one thing, the data is points are geomasked. Th means that points are highly likely to be overlapping, giving a skewed picture of the distribution. There are ways round this, such as through jittering or applying census based data.



```{r}
pop <- read_csv("Data/Census Population/Data_UNIT_URESPOP.csv") %>% slice(3:57) %>% 
  select(2,3,6,7, 8) %>%
  janitor::clean_names() %>%
  rename(pop_density = f2383, 
         pop_count_wrk = f2384, 
         pop_count_res = f323339) %>% 
  mutate_at(c('pop_density', 'pop_count_wrk',
              'pop_count_res'), as.numeric)
```



Again we join this to our surrey_lsoa file, by matching the LSOAs


```{r}
surrey_lsoa <- left_join(surrey_lsoa, pop, by = c("code"="geo_code"))

```



Now you will see the census data has merged into the shapefile, 

A crime rate is calculated by dividing the number of reported by the total population, and then multiplied by 100,000. 

In this case that would be the count variable, divided by the 'pop' variable, and then times by 1000 (we use 1000 as this is the average population of an LSOA, if you were using larger UoA you can choose to multiply by 100,000. Just remember what affect this will have on your rate and how this then interpreted across your results)

In order to work out the crime rate, we need to create a new variable that takes the count/pop*10000

```{r}
surrey_lsoa <- surrey_lsoa %>% 
  mutate(crime_rate = (count/pop_count_wrk)*1000)

```




Now lets explore these trends using ggplot and tmap 

##ggplot

```{r}
ggplot() + 
  annotation_map_tile() + 
  geom_sf(data = surrey_lsoa, aes(fill = crime_rate), alpha = 0.5) + 
  scale_fill_gradient2(name ="Crime Rate")
```


##tmaps 

```{r}
tm_shape(surrey_lsoa) + 
  tm_fill("crime_rate", style = "quantile") + 
  tm_borders(alpha = 0.3)
```




## Cartograms and ggplot

A cartogram is a type of map where different geographic areas are modified based on a variable associated to those areas. 

Two types: contiguous vs non-contiguous (sharing a common border) 


```{r}
library(cartogram)

#In our data set we have a variable “pop_count” which refers to the total number of people in our LSOA 

cart <- cartogram_cont(surrey_lsoa, weight = "pop_count_wrk")   

## simple plot 
ggplot(cart) + 
  geom_sf()

## fill with our count variable 
ggplot(cart) + 
  geom_sf(aes(fill = pop_count_wrk))

## add in some aesthetics 
ggplot(cart) + 
  geom_sf(aes(fill = pop_count_wrk), 
          color = "gray50", 
          linetype = 1, 
          lwd = 0.35) + 
  scale_fill_gradientn(colours = heat.colors(n =10, 
                                            alpha = 0.5, 
                                            rev = TRUE)) + 
  theme_gray() + 
  labs(title = "Surrey Heath: Population by LSOA", 
       subtitle = "August 2020")
```







## Activity 3

We have mapped the variable pop_count_wrk, now lets do the same with the variable pop_count_res

Steps: 
- 1) First calculate the crime rate 
- 2) Plot using ggplot 
- 3) Plot using tmap 
- 4) Plot both maps together using tmap_arrange 
- 5) Plot a cartogram of residential population

Is there a difference between the crime rate when using workday population compared to residential population? Would we expect to see these trends




```{r}
# 1) First calculate the crime rate 

surrey_lsoa <- surrey_lsoa %>% 
  mutate(crime_rate2 = (count/pop_count_res)*1000)


#2) Plot using ggplot 

ggplot() + 
  annotation_map_tile() + 
  geom_sf(data = surrey_lsoa, aes(fill = crime_rate2), alpha = 0.5) + 
  scale_fill_gradient2(name ="Crime Rate")


#3) Plot using tmap 

tm_shape(surrey_lsoa) + 
  tm_fill("crime_rate", style = "quantile") + 
  tm_borders(alpha = 0.3)


#4) Compare the workday vs residential population 

e <- tm_shape(surrey_lsoa) + 
  tm_fill("crime_rate", style = "quantile", title = "Workday Pop") + 
  tm_borders(alpha = 0.3)

f <- tm_shape(surrey_lsoa) + 
  tm_fill("crime_rate2", style = "quantile", title = "Residential pop") + 
  tm_borders(alpha = 0.3)


tmap_arrange(e, f)


#5) Cartogram

ggplot(cart) + 
  geom_sf(aes(fill = pop_count_res), 
          color = "gray50", 
          linetype = 1, 
          lwd = 0.35) + 
  scale_fill_gradientn(colours = heat.colors(n =10, 
                                            alpha = 0.5, 
                                            rev = TRUE)) + 
  theme_gray() + 
  labs(title = "Surrey Heath: Population by LSOA", 
       subtitle = "August 2020")


```
 







# Topic 4 - Additional Material



### Binning data

Binning, can be thought of as a two-dimensional histogram (shading of the bins take the heights of the bars). 

Need to convert the sf data.frame geometry column into a data.frame with separate x, y columns 

How do you separate the coordinates? 

https://github.com/r-spatial/sf/issues/231 

```{r}
sfc_as_cols <- function(x, names = c("x","y")) {
  stopifnot(inherits(x,"sf") && inherits(sf::st_geometry(x),"sfc_POINT"))
  ret <- sf::st_coordinates(x)
  ret <- tibble::as_tibble(ret)
  stopifnot(length(names) == raster::ncol(ret))
  x <- x[ , !names(x) %in% names]
  ret <- setNames(ret,names)
  dplyr::bind_cols(x,ret)
}

sf_seperate <- sfc_as_cols(sf, c("lng", "lat")) 


ggplot(sf_seperate, aes(lng, lat)) +   
  annotation_map_tile() +
  stat_binhex(bins = 30) +                                           
  scale_fill_gradientn(colours = c("white","red"), name = "Frequency")   


#hexagonal = stat_binhex() 
#rectangle = stat_bin2d()
#heat = stat_density2d()  

```





### Interactive Maps; Leaflet

```{r}

## Subsetting for just ASB 
asb <- subset(crime01_19, crime_type == "Anti-social behaviour")

m <- leaflet(data = asb) %>%
  addProviderTiles("Stamen.Toner") %>% 
  addMarkers(lng=~longitude, lat=~latitude, popup=~as.character(location), label = ~as.character(location))
m

```














